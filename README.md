# ğŸš€ MCP Server for Document Processing

This project implements a Model Context Protocol (MCP) server that processes Markdown and text files, chunks and tokenizes the content using embedding models, and makes this processed content available through MCP tools.

## ğŸ—ï¸ Architecture

The system consists of two main components:

1. **ğŸ“ Processing Pipeline**: Reads Markdown and text files, chunks them, generates embeddings, and stores them in a vector database.
2. **ğŸ”Œ MCP Server**: Exposes the processed content through MCP tools, allowing Roo Code to search and retrieve relevant information.

## âœ… Prerequisites

- Docker and Docker Compose
- OpenAI API key (required for embeddings unless using a custom embedding function)
- Anthropic API key (optional, for Claude response generation)

## ğŸ› ï¸ Setup

1. Clone this repository:
   ```bash
   git clone https://github.com/donphi/mcp-server.git
   cd mcp-server
   ```

2. Create a `.env` file with your configuration:
   ```
   # Copy the example file
   cp .env.example .env
   
   # Edit the file with your settings
   nano .env
   ```

3. Place your Markdown (.md) and text (.txt) files in the `data/` directory.

## âš™ï¸ Configuration

You can configure the MCP server using environment variables in the `.env` file:

```
# API Keys
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here  # Optional

# Pipeline Configuration
CHUNK_SIZE=1000                # Size of text chunks
CHUNK_OVERLAP=200              # Overlap between chunks (in tokens)
BATCH_SIZE=10                  # Batch size for embedding generation
EMBEDDING_MODEL=text-embedding-ada-002  # OpenAI embedding model to use
SUPPORTED_EXTENSIONS=.md,.txt  # Comma-separated list of supported file extensions

# Server Configuration
CLAUDE_MODEL=claude-3-7-sonnet-20240307  # Claude model to use
MAX_RESULTS=10                 # Maximum number of results to return
USE_ANTHROPIC=true             # Whether to use Anthropic API for responses

# Custom Embedding Model (optional)
# CUSTOM_EMBEDDING_MODULE=/path/to/your/module.py
# CUSTOM_EMBEDDING_FUNCTION=your_embedding_function

# Paths
DATA_DIR=/data                 # Directory containing input files
OUTPUT_DIR=/output             # Directory for output files
DB_PATH=/db                    # Directory for vector database
CONFIG_PATH=/config/server_config.json  # Path to server configuration file
```

## ğŸš€ Usage

### ğŸ”„ Processing the Files

To process your files and generate embeddings:

```bash
docker-compose build pipeline
docker-compose run pipeline
```

This will:
- Read all supported files in the `data/` directory
- Process and chunk the content
- Generate embeddings
- Store the embeddings in the vector database

### ğŸƒâ€â™‚ï¸ Running the MCP Server

To start the MCP server:

```bash
docker-compose build server
docker-compose up -d server
```

### ğŸ”— Connecting to Roo Code

Configure Roo Code to connect to the MCP server by adding the following to your Roo Code configuration:

```json
{
  "mcpServers": {
    "mcp-server": {
      "command": "docker",
      "args": [
        "exec",
        "-i",
        "mcp_server_1",
        "python",
        "server.py"
      ]
    }
  }
}
```

## ğŸ§° MCP Tools

The MCP server exposes the following tools:

- **ğŸ“š read-md-files**: Process and retrieve files
- **ğŸ” search-content**: Search across processed content
- **ğŸ“‹ get-context**: Retrieve contextual information
- **ğŸ—ï¸ project-structure**: Provide project structure information
- **ğŸ’¡ suggest-implementation**: Generate implementation suggestions

## ğŸ“„ Supported File Types

By default, the following file types are supported:
- Markdown files (.md)
- Text files (.txt)

You can configure additional file extensions by setting the `SUPPORTED_EXTENSIONS` environment variable in your `.env` file.

## ğŸ”„ Operational Modes

The MCP server can operate in two modes:

1. **ğŸ¤– Full Processing Mode**: When the Anthropic API key is provided and `USE_ANTHROPIC` is set to `true`, the server will use Claude to generate responses based on the retrieved context.

2. **ğŸ“‹ Context Retrieval Mode**: When the Anthropic API key is not provided or `USE_ANTHROPIC` is set to `false`, the server will only retrieve and return the relevant context, allowing the client (e.g., Roo Code) to process it using its own LLM.

## ğŸ”§ Custom Embedding Models

You can use your own embedding model instead of OpenAI's by implementing a custom embedding function:

1. Create a Python module with your embedding function:

```python
# custom_embeddings.py

def my_embedding_function(texts):
    """
    Generate embeddings for a list of texts.
    
    Args:
        texts: List of texts to embed
        
    Returns:
        List of embeddings (each embedding is a list of floats)
    """
    # Your embedding logic here
    # ...
    return embeddings  # List of embeddings
```

2. Set the environment variables in your `.env` file:

```
CUSTOM_EMBEDDING_MODULE=/path/to/custom_embeddings.py
CUSTOM_EMBEDDING_FUNCTION=my_embedding_function
```

## ğŸ“ Project Structure

```
mcp-server/
â”œâ”€â”€ Dockerfile.pipeline
â”œâ”€â”€ Dockerfile.server
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.pipeline.txt
â”œâ”€â”€ requirements.server.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .env.example
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ server.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ chunking.py
â”‚       â”œâ”€â”€ embedding.py
â”‚       â””â”€â”€ vector_db.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ pipeline_config.json
â”‚   â””â”€â”€ server_config.json
â”œâ”€â”€ data/
â”œâ”€â”€ output/
â””â”€â”€ db/
```

## ğŸ“„ License

MIT

---

Created with â¤ï¸ by [donphi](https://github.com/donphi)